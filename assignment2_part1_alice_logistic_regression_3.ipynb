{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Import libraries and set desired options\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/kashnitsky/correct-time-aware-cross-validation-scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "train_df = pd.read_csv('alice\\\\train_sessions.csv',\n",
    "                       index_col='session_id', parse_dates=times)\n",
    "test_df = pd.read_csv('alice\\\\test_sessions.csv',\n",
    "                      index_col='session_id', parse_dates=times)\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "# Look at the first rows of the training set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform data into format which can be fed into CountVectorizer\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[sites].fillna(0).astype('int').to_csv('train_sessions_text.txt', \n",
    "                                               sep=' ', \n",
    "                       index=None, header=None)\n",
    "test_df[sites].fillna(0).astype('int').to_csv('test_sessions_text.txt', \n",
    "                                              sep=' ', \n",
    "                       index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Fit CountVectorizer and trasfrom data with it\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1, 3), max_features=60000)\n",
    "with open('train_sessions_text.txt') as inp_train_file:\n",
    "    X_train = cv.fit_transform(inp_train_file)\n",
    "with open('test_sessions_text.txt') as inp_test_file:\n",
    "    X_test = cv.transform(inp_test_file)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save train targets into a separate vector.\n",
    "y_train = train_df['target'].astype('int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll be performing time series cross-validation, see sklearn TimeSeriesSplit and this dicussion on StackOverflow\n",
    "time_split = TimeSeriesSplit(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((23051,), (23051,)),\n",
       " ((46102,), (23051,)),\n",
       " ((69153,), (23051,)),\n",
       " ((92204,), (23051,)),\n",
       " ((115255,), (23051,)),\n",
       " ((138306,), (23051,)),\n",
       " ((161357,), (23051,)),\n",
       " ((184408,), (23051,)),\n",
       " ((207459,), (23051,)),\n",
       " ((230510,), (23051,))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "[(el[0].shape, el[1].shape) for el in time_split.split(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform time series cross-validation with logistic regression\n",
    "logit = LogisticRegression(C=1, random_state=17, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_scores = cross_val_score(logit, X_train, y_train, cv=time_split, \n",
    "                            scoring='roc_auc', n_jobs=-1) # hangs with n_jobs > 1, and locally this runs much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.82427331, 0.64549526, 0.8820242 , 0.96012223, 0.84373089,\n",
       "        0.87848007, 0.94454016, 0.85417534, 0.93131298, 0.90788391]),\n",
       " 0.8672038343890842)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=17, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train logistic regression with all training data, make predictions for test set and form a submission file\n",
    "logit.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit.predict_proba(X_test)[:, 1]\n",
    "write_to_submission_file(logit_test_pred, 'subm1.csv') # 0.91288\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll add some time features: indicators of morning, day, evening and night.\n",
    "\n",
    "def add_time_features(df, X_sparse):\n",
    "    hour = df['time1'].apply(lambda ts: ts.hour)\n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "    X = hstack([X_sparse, morning.values.reshape(-1, 1), \n",
    "                day.values.reshape(-1, 1), evening.values.reshape(-1, 1), \n",
    "                night.values.reshape(-1, 1)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_new = add_time_features(train_df.fillna(0), X_train)\n",
    "X_test_new = add_time_features(test_df.fillna(0), X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 60004), (82797, 60004))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape, X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Performing time series cross-validation, we see an improvement in ROC AUC.\n",
    "cv_scores = cross_val_score(logit, X_train_new, y_train, cv=time_split, \n",
    "                            scoring='roc_auc', n_jobs=-1) # hangs with n_jobs > 1, and locally this runs much faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.87283494, 0.7555006 , 0.93147398, 0.97665104, 0.90554704,\n",
       "        0.93879362, 0.96194229, 0.92844498, 0.95      , 0.94082052]),\n",
       " 0.9162009010392771)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=17, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making a new submission, we notice a leaderboard score improvement as well (0.91288 -> 0.93843). Correlated CV and LB improvements is a good justifications for added features being useful and CV scheme being correct.\n",
    "logit.fit(X_train_new, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred2 = logit.predict_proba(X_test_new)[:, 1]\n",
    "write_to_submission_file(logit_test_pred2, 'subm2.csv') # 0.93843\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we tune regularization parameter C.\n",
    "c_values = np.logspace(-2, 2, 10)\n",
    "logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n",
    "                                  scoring='roc_auc', n_jobs=1, cv=time_split, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 17.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=17, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([1.00000e-02, 2.78256e-02, 7.74264e-02, 2.15443e-01, 5.99484e-01,\n",
       "       1.66810e+00, 4.64159e+00, 1.29155e+01, 3.59381e+01, 1.00000e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit_grid_searcher.fit(X_train_new, y_train) # WTF? Locally, it's 3min 30s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.917376467814612, {'C': 0.21544346900318834})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_grid_searcher.best_score_, logit_grid_searcher.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred3 = logit_grid_searcher.predict_proba(X_test_new)[:, 1]\n",
    "write_to_submission_file(logit_test_pred3, 'subm3.csv') # 0.94242\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "popular_alice_sites = [77, 80, 76, 29, 21, 81, 879, 22, 75, 82, 23, 35, 881, 37, 33, 3000, 733, 30, 78, 941, 7832]\n",
    "intersection_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_alice_sites = [77, 80, 76, 29, 21, 81, 879, 22, 75, 82, 23, 35, 881, 37, 33, 3000, 733, 30, 78, 941, 7832]\n",
    "intersection_train = ((train_df['site1'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     train_df['site2'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     train_df['site3'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     train_df['site4'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     train_df['site5'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     train_df['site6'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     train_df['site7'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     train_df['site8'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     train_df['site9'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     train_df['site10'].apply(lambda s: s in popular_alice_sites).astype('float64'))>9).astype('float64')\n",
    "intersection_test = ((test_df['site1'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     test_df['site2'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     test_df['site3'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     test_df['site4'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     test_df['site5'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     test_df['site6'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     test_df['site7'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     test_df['site8'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     test_df['site9'].apply(lambda s: s in popular_alice_sites).astype('float64') +\\\n",
    "                     test_df['site10'].apply(lambda s: s in popular_alice_sites).astype('float64'))>9).astype('float64')\n",
    "intersection_train = StandardScaler().fit_transform(intersection_train.values.reshape(-1,1))\n",
    "intersection_test = StandardScaler().fit_transform(intersection_test.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#add start_month feature\n",
    "start_month_train = train_df['time1'].apply(lambda ts:100 * ts.year + ts.month).astype('float64')\n",
    "start_month_train = StandardScaler().fit_transform(start_month_train.values.reshape(-1,1))\n",
    "X_train_mod = hstack([X_train_new, start_month_train])\n",
    "start_month_test = test_df['time1'].apply(lambda ts:100 * ts.year + ts.month).astype('float64')\n",
    "start_month_test = StandardScaler().fit_transform(start_month_test.values.reshape(-1,1))\n",
    "X_test_mod = hstack([X_test_new, start_month_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#full_df['duration'] = full_df[times].max(axis=1)-full_df[times].min(axis=1)\n",
    "#full_df['duration']=full_df['duration'].apply(lambda ts:ts.seconds).astype('float64')\n",
    "duration_train = train_df[times].max(axis=1)-train_df[times].min(axis=1)\n",
    "duration_test = test_df[times].max(axis=1)-test_df[times].min(axis=1)\n",
    "duration_train = duration_train.apply(lambda ts:ts.seconds).astype('float64')\n",
    "duration_test = duration_test.apply(lambda ts:ts.seconds).astype('float64')\n",
    "duration_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(duration_train) #= StandardScaler().fit_transform(duration_train.values.reshape(-1,1))\n",
    "duration_train = StandardScaler().fit_transform(duration_train.values.reshape(-1,1))\n",
    "duration_test = StandardScaler().fit_transform(duration_test.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 60007), (82797, 60007))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mod = hstack([X_train_new, start_month_train, duration_train, intersection_train])\n",
    "X_test_mod = hstack([X_test_new, start_month_test, duration_test, intersection_test])\n",
    "X_train_mod.shape, X_test_mod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Performing time series cross-validation, we see an improvement in ROC AUC.\n",
    "cv_scores = cross_val_score(logit, X_train_mod, y_train, cv=time_split, \n",
    "                            scoring='roc_auc', n_jobs=-1) # hangs with n_jobs > 1, and locally this runs much faster\n",
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.87949428, 0.76634631, 0.95092091, 0.97799922, 0.90702422,\n",
       "        0.94191652, 0.96419839, 0.92952671, 0.95111928, 0.94171334]),\n",
       " 0.9210259178569732)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores, cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=17, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit.fit(X_train_mod, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred2 = logit.predict_proba(X_test_mod)[:, 1]\n",
    "write_to_submission_file(logit_test_pred2, 'subm2.csv') # 0.94187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we tune regularization parameter C.\n",
    "c_values = np.logspace(-2, 2, 10)\n",
    "logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n",
    "                                  scoring='roc_auc', n_jobs=-1, cv=time_split, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 29.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=17, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'C': array([1.00000e-02, 2.78256e-02, 7.74264e-02, 2.15443e-01, 5.99484e-01,\n",
       "       1.66810e+00, 4.64159e+00, 1.29155e+01, 3.59381e+01, 1.00000e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit_grid_searcher.fit(X_train_mod, y_train) # WTF? Locally, it's 3min 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9230989398081443, {'C': 0.21544346900318834})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_grid_searcher.best_score_, logit_grid_searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred3 = logit_grid_searcher.predict_proba(X_test_mod)[:, 1]\n",
    "write_to_submission_file(logit_test_pred3, 'subm3.csv') # 0.94589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
